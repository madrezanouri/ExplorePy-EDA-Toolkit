{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def import_local_file(file_path, file_type, **kwargs):\n",
    "    \"\"\"\n",
    "    Import data from local files.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file.\n",
    "        file_type (str): Type of the file ('csv', 'excel', 'json', 'parquet', etc.).\n",
    "        **kwargs: Additional arguments for specific file readers.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    if file_type == 'csv':\n",
    "        df = pd.read_csv(file_path, **kwargs)\n",
    "    elif file_type == 'excel':\n",
    "        df = pd.read_excel(file_path, **kwargs)\n",
    "    elif file_type == 'json':\n",
    "        df = pd.read_json(file_path, **kwargs)\n",
    "    elif file_type == 'parquet':\n",
    "        df = pd.read_parquet(file_path, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df_csv = import_local_file('data.csv', 'csv')\n",
    "\n",
    "# Excel\n",
    "df_excel = import_local_file('data.xlsx', 'excel')\n",
    "\n",
    "# JSON\n",
    "df_json = import_local_file('data.json', 'json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def import_from_sql(query, db_path):\n",
    "    \"\"\"\n",
    "    Import data from an SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        query (str): SQL query to execute.\n",
    "        db_path (str): Path to the SQLite database.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Resulting dataset.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Query\n",
    "query = \"SELECT * FROM employees WHERE salary > 50000\"\n",
    "df_sql = import_from_sql(query, 'database.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abe4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def import_from_api(url, headers=None, params=None):\n",
    "    \"\"\"\n",
    "    Import data from an API endpoint.\n",
    "    \n",
    "    Args:\n",
    "        url (str): API endpoint URL.\n",
    "        headers (dict): Headers for the API request.\n",
    "        params (dict): Query parameters for the API request.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Data from the API as a DataFrame.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Assuming the API returns JSON\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example API\n",
    "api_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "df_api = import_from_api(api_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_large_csv(file_path, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Import large CSV files in chunks.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        chunk_size (int): Number of rows per chunk.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset after reading in chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e20da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = import_large_csv('large_file.csv', chunk_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271da984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "def import_from_s3(bucket_name, file_key, aws_access_key, aws_secret_key):\n",
    "    \"\"\"\n",
    "    Import data from an AWS S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "        file_key (str): Key (path) to the file in the bucket.\n",
    "        aws_access_key (str): AWS access key.\n",
    "        aws_secret_key (str): AWS secret key.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    data = obj['Body'].read().decode('utf-8')\n",
    "    return pd.read_csv(StringIO(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516dc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = import_from_s3(\n",
    "    bucket_name='my-bucket',\n",
    "    file_key='path/to/data.csv',\n",
    "    aws_access_key='your-access-key',\n",
    "    aws_secret_key='your-secret-key'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83de413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_summary(df):\n",
    "    \"\"\"\n",
    "    Display basic information about the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset.\n",
    "    \"\"\"\n",
    "    print(\"Shape of dataset:\", df.shape)\n",
    "    print(\"\\nInfo:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a831fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary(df_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    \"\"\"\n",
    "    Reduce memory usage by optimizing data types.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataset.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Optimized dataset.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='unsigned')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeceae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized = optimize_memory(df_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
